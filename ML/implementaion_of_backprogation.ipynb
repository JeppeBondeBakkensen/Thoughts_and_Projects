{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f170fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6931442641906013\n",
      "probs: [[0.49998158 0.50077793 0.49920365 0.49999111]]\n",
      "preds: [[0 1 0 0]]\n",
      "true : [[0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Write shapes everywhere needed \n",
    " \n",
    "layer_sizes = [2, 3, 1]\n",
    "num_layers = len(layer_sizes)\n",
    "\n",
    "\n",
    "mu, sigma = 0, 0.1  # mean and standard deviation\n",
    "weights = [] \n",
    "biases = []\n",
    "\n",
    "# Generate start matrix for weights and biases \n",
    "for i in range(num_layers - 1):\n",
    "    n_in = layer_sizes[i]           # shape 2x3\n",
    "    n_out = layer_sizes[i + 1]      # shape 3x1\n",
    "    weights.append(np.random.normal(mu, sigma, size=(n_out, n_in)))\n",
    "    biases.append(np.random.normal(mu, sigma, size=(n_out, 1)))\n",
    "\n",
    "# Activation function \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ðž‚'(z) \n",
    "def sigmoid_prime(z): \n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Cost function \n",
    "def bce_loss(y, a, eps=1e-12):\n",
    "    # y, a: (1, m)\n",
    "    a = np.clip(a, a_min=eps, a_max=1-eps)  # Ensures that a never becomes 0\n",
    "    m = y.shape[1]  # Number of inputs \n",
    "    return -1/m * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "\n",
    "# XOR input data\n",
    "x = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "def feedforward(X, weights, biases):\n",
    "    A = X           # A^(0)\n",
    "    A_list = [X]    # stores activations\n",
    "    Z_list = []     # stores pre-activations\n",
    "    \n",
    "    for l in range(len(weights)): \n",
    "        Z = weights[l] @ A + biases[l]\n",
    "        A = sigmoid(Z)\n",
    "        Z_list.append(Z)\n",
    "        A_list.append(A)\n",
    "    return A_list, Z_list\n",
    "\n",
    "def update_params(weights, biases, nabla_w, nabla_b, lr):\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] -= lr * nabla_w[l]\n",
    "        biases[l]  -= lr * nabla_b[l]\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "lr = 1.0\n",
    "epochs = 100\n",
    "for epch in range(epochs):     \n",
    "    A_list, Z_list = feedforward(X=x, weights=weights, biases=biases)\n",
    "    m = x.shape[1]\n",
    "    loss = bce_loss(y, A_list[-1])\n",
    "    # BP1: Compute delta for output layer \n",
    "    delta_L = A_list[-1] - y\n",
    "\n",
    "    # Store deltas backwards \n",
    "    deltas = [delta_L]\n",
    "\n",
    "    # BP2: Backpropagate trough hidden layers \n",
    "    for l in range(len(weights) - 1, 0, -1): \n",
    "        delta_next = deltas[0]  # Most recently computed delta (from layer l+1)\n",
    "        z = Z_list[l - 1]       # Pre-activation at layer l\n",
    "        delta = (weights[l].T @ delta_next) * sigmoid_prime(z)\n",
    "        deltas.insert(0, delta) # Insert at the beginning\n",
    "\n",
    "    # Store gradients\n",
    "    nable_weights, nabla_biases = [], []\n",
    "\n",
    "    for l in range(len(weights)):\n",
    "        nable_weights.append((deltas[l] @ A_list[l].T) / m)                 # matches weights[l]\n",
    "        nabla_biases.append(np.sum(deltas[l], axis=1, keepdims=True) / m)  # matches biases[l]\n",
    "        \n",
    "    weights, biases = update_params(weights, biases, nable_weights, nabla_biases, lr)\n",
    "        \n",
    "    if epch % 100 == 0: \n",
    "        print(epch, loss)\n",
    "    \n",
    "# Evaluate \n",
    "A_list, _ = feedforward(X=x, weights=weights, biases=biases)\n",
    "probs = A_list[-1]\n",
    "preds = (probs > 0.5).astype(int)\n",
    "\n",
    "print(\"probs:\", probs)\n",
    "print(\"preds:\", preds)\n",
    "print(\"true :\", y)\n",
    "\n",
    "# y_hat = A_list[-1]\n",
    "# print(f\"Output shpae y_hat: {y_hat.shape}\")\n",
    "# print(f\"Output of y_hat: {y_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56674e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifer: \n",
    "    def __init__(self, layer_sizes, mu=0.0, sigma=0.01): \n",
    "        self.layer_sizes = layer_sizes \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma \n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        \n",
    "        # Generate start matrix for weights and biases \n",
    "        for i in range(num_layers - 1):\n",
    "            n_in = self.layer_sizes[i]           \n",
    "            n_out = self.layer_sizes[i + 1]      \n",
    "            self.weights.append(np.random.normal(mu, sigma, size=(n_out, n_in)))\n",
    "            self.biases.append(np.random.normal(mu, sigma, size=(n_out, 1)))\n",
    "\n",
    "        self.A_list = None \n",
    "        self.Z_list = None \n",
    "        self.loss_history = []\n",
    "    \n",
    "    # -- Activation function --- \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self, z): \n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    # --- loss function ---\n",
    "    def bce_loss(self, y, a, eps=1e-12):\n",
    "        # y, a: (1, m)\n",
    "        a = np.clip(a, a_min=eps, a_max=1-eps)  # Ensures that a never becomes 0\n",
    "        m = y.shape[1]  # Number of inputs \n",
    "        return -1/m * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "\n",
    "    # --- Feedforward--- \n",
    "    def feedforward(self, X):\n",
    "        A = X           # A^(0)\n",
    "        A_list = [X]    # stores activations\n",
    "        Z_list = []     # stores pre-activations\n",
    "        \n",
    "        for l in range(len(weights)): \n",
    "            Z = self.weights[l] @ A + self.biases[l]\n",
    "            A = sigmoid(Z)\n",
    "            Z_list.append(Z)\n",
    "            A_list.append(A)\n",
    "            \n",
    "            self.A_list, self.Z_list =A_list, Z_list\n",
    "        return A\n",
    "    \n",
    "    # --- Backward propagation --- \n",
    "    def backward(self, y): \n",
    "        A_list, Z_list = feedforward(X=x, weights=weights, biases=biases)\n",
    "        m = x.shape[1]\n",
    "\n",
    "        # BP1: Compute delta for output layer \n",
    "        delta_L = A_list[-1] - y\n",
    "\n",
    "        # Store deltas backwards \n",
    "        deltas = [delta_L]\n",
    "\n",
    "        # BP2: Backpropagate trough hidden layers \n",
    "        for l in range(len(weights) - 1, 0, -1): \n",
    "            delta_next = deltas[0]  # Most recently computed delta (from layer l+1)\n",
    "            z = Z_list[l - 1]       # Pre-activation at layer l\n",
    "            delta = (weights[l].T @ delta_next) * sigmoid_prime(z)\n",
    "            deltas.insert(0, delta) # Insert at the beginning\n",
    "\n",
    "        # Store gradients\n",
    "        nable_weights, nabla_biases = [], []\n",
    "\n",
    "        for l in range(len(weights)):\n",
    "            nable_weights.append((deltas[l] @ A_list[l].T) / m)                 # matches weights[l]\n",
    "            nabla_biases.append(np.sum(deltas[l], axis=1, keepdims=True) / m)  # matches biases[l]\n",
    "        return nable_weights, nabla_biases\n",
    "    \n",
    "    # --- update --- \n",
    "    def step(self, nabla_w, nabla_b, lr):\n",
    "        for l in range(len(self.weights)):\n",
    "            self.weights[l] -= lr * nabla_w[l]\n",
    "            self.biases[l]  -= lr * nabla_b[l]\n",
    "    \n",
    "    # --- training --- \n",
    "    def fit(self, X, y, lr=1.0, epochs=1000, verbose=100): \n",
    "        for epoch in range(epochs): \n",
    "            yhat = self.feedforward(X)\n",
    "            loss = self.bce_loss(y, yhat)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            nabla_w, nabla_b = self.backward(y)\n",
    "            self.step(nabla_w, nabla_b, lr)\n",
    "            if verbose is not None and epoch % verbose == 0:\n",
    "                print(epoch, loss)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X): \n",
    "        return self.feedforward(X)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) > threshold).astype(int)\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
