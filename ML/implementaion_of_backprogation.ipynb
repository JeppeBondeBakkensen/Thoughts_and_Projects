{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56674e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BinaryClassifier: \n",
    "    def __init__(self, layer_sizes, mu=0.0, sigma=0.01): \n",
    "        self.layer_sizes = layer_sizes \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma \n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        \n",
    "        # Generate start matrix for weights and biases \n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            n_in = self.layer_sizes[i]           \n",
    "            n_out = self.layer_sizes[i + 1]      \n",
    "            self.weights.append(np.random.normal(mu, sigma, size=(n_out, n_in)))\n",
    "            self.biases.append(np.random.normal(mu, sigma, size=(n_out, 1)))\n",
    "\n",
    "        self.A_list = None \n",
    "        self.Z_list = None \n",
    "        self.loss_history = []\n",
    "    \n",
    "    # -- Activation function --- \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self, z): \n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    # --- loss function ---\n",
    "    def bce_loss(self, y, a, eps=1e-12):\n",
    "        # y, a: (1, m)\n",
    "        a = np.clip(a, a_min=eps, a_max=1-eps)  # Ensures that a never becomes 0\n",
    "        m = y.shape[1]  # Number of inputs \n",
    "        return -1/m * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "\n",
    "    # --- Feedforward--- \n",
    "    def feedforward(self, X):\n",
    "        A = X           # A^(0)\n",
    "        A_list = [X]    # stores activations\n",
    "        Z_list = []     # stores pre-activations\n",
    "        \n",
    "        for l in range(len(weights)): \n",
    "            Z = self.weights[l] @ A + self.biases[l]\n",
    "            A = sigmoid(Z)\n",
    "            Z_list.append(Z)\n",
    "            A_list.append(A)\n",
    "            \n",
    "            self.A_list, self.Z_list =A_list, Z_list\n",
    "        return A\n",
    "    \n",
    "    # --- Backward propagation --- \n",
    "    def backward(self, y): \n",
    "        A_list, Z_list = self.A_list, self.Z_list\n",
    "        m = y.shape[1]\n",
    "\n",
    "        # BP1: Compute delta for output layer \n",
    "        delta_L = A_list[-1] - y\n",
    "\n",
    "        # Store deltas backwards \n",
    "        deltas = [delta_L]\n",
    "\n",
    "        # BP2: Backpropagate trough hidden layers \n",
    "        for l in range(len(self.weights) - 1, 0, -1): \n",
    "            delta_next = deltas[0]  # Most recently computed delta (from layer l+1)\n",
    "            z = Z_list[l - 1]       # Pre-activation at layer l\n",
    "            delta = (self.weights[l].T @ delta_next) * self.sigmoid_prime(z)\n",
    "            deltas.insert(0, delta) # Insert at the beginning\n",
    "\n",
    "        # Store gradients\n",
    "        nable_weights, nabla_biases = [], []\n",
    "\n",
    "        for l in range(len(self.weights)):\n",
    "            nable_weights.append((deltas[l] @ A_list[l].T) / m)                 # matches weights[l]\n",
    "            nabla_biases.append(np.sum(deltas[l], axis=1, keepdims=True) / m)  # matches biases[l]\n",
    "        return nable_weights, nabla_biases\n",
    "    \n",
    "    # --- update --- \n",
    "    def step(self, nabla_w, nabla_b, lr):\n",
    "        for l in range(len(self.weights)):\n",
    "            self.weights[l] -= lr * nabla_w[l]\n",
    "            self.biases[l]  -= lr * nabla_b[l]\n",
    "    \n",
    "    # --- training --- \n",
    "    def fit(self, X, y, lr=1.0, epochs=1000, verbose=100): \n",
    "        for epoch in range(epochs): \n",
    "            yhat = self.feedforward(X)\n",
    "            loss = self.bce_loss(y, yhat)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            nabla_w, nabla_b = self.backward(y)\n",
    "            self.step(nabla_w, nabla_b, lr)\n",
    "            if verbose is not None and epoch % verbose == 0:\n",
    "                print(epoch, loss)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X): \n",
    "        return self.feedforward(X)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89b5174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6932973185277789\n",
      "500 0.6931471805989539\n",
      "1000 0.6931471805984111\n",
      "1500 0.6931471805978672\n",
      "2000 0.693147180597322\n",
      "2500 0.6931471805967759\n",
      "3000 0.6931471805962288\n",
      "3500 0.6931471805956804\n",
      "4000 0.6931471805951309\n",
      "4500 0.6931471805945804\n",
      "probs: [[0.49998965 0.49999819 0.50000181 0.50001035]]\n",
      "preds: [[0 0 1 1]]\n",
      "true : [[0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1, 1],\n",
    "              [0, 1, 0, 1]])\n",
    "y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "net = BinaryClassifier([2, 3, 1])\n",
    "net.fit(X, y, lr=1.0, epochs=5000, verbose=500)\n",
    "\n",
    "probs = net.predict_proba(X)\n",
    "preds = net.predict(X)\n",
    "\n",
    "print(\"probs:\", probs)\n",
    "print(\"preds:\", preds)\n",
    "print(\"true :\", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
